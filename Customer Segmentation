{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74935,"sourceType":"datasetVersion","datasetId":42674}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/xenowing/task-02-customer-segmentation-python?scriptVersionId=183946108\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T08:23:35.317419Z","iopub.execute_input":"2024-06-17T08:23:35.317812Z","iopub.status.idle":"2024-06-17T08:23:36.499426Z","shell.execute_reply.started":"2024-06-17T08:23:35.31778Z","shell.execute_reply":"2024-06-17T08:23:36.498306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task- Create k-means clusttering algorithm to group customers of a retail store based on thier purchases history","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport tensorflow as tf\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:36.501245Z","iopub.execute_input":"2024-06-17T08:23:36.501694Z","iopub.status.idle":"2024-06-17T08:23:50.457821Z","shell.execute_reply.started":"2024-06-17T08:23:36.501665Z","shell.execute_reply":"2024-06-17T08:23:50.456673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.459003Z","iopub.execute_input":"2024-06-17T08:23:50.459599Z","iopub.status.idle":"2024-06-17T08:23:50.479336Z","shell.execute_reply.started":"2024-06-17T08:23:50.459568Z","shell.execute_reply":"2024-06-17T08:23:50.478131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.482218Z","iopub.execute_input":"2024-06-17T08:23:50.482742Z","iopub.status.idle":"2024-06-17T08:23:50.520343Z","shell.execute_reply.started":"2024-06-17T08:23:50.482698Z","shell.execute_reply":"2024-06-17T08:23:50.519064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.521632Z","iopub.execute_input":"2024-06-17T08:23:50.521955Z","iopub.status.idle":"2024-06-17T08:23:50.532773Z","shell.execute_reply.started":"2024-06-17T08:23:50.521927Z","shell.execute_reply":"2024-06-17T08:23:50.531534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.534116Z","iopub.execute_input":"2024-06-17T08:23:50.534496Z","iopub.status.idle":"2024-06-17T08:23:50.543394Z","shell.execute_reply.started":"2024-06-17T08:23:50.534464Z","shell.execute_reply":"2024-06-17T08:23:50.542318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.544843Z","iopub.execute_input":"2024-06-17T08:23:50.545747Z","iopub.status.idle":"2024-06-17T08:23:50.558331Z","shell.execute_reply.started":"2024-06-17T08:23:50.545714Z","shell.execute_reply":"2024-06-17T08:23:50.557218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.scatter(dataset['Age'],dataset['Spending Score (1-100)'],c='blue',alpha=0.5)\nplt.title(\"Age vs Spending Score\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Spending Score\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.559723Z","iopub.execute_input":"2024-06-17T08:23:50.560072Z","iopub.status.idle":"2024-06-17T08:23:50.8533Z","shell.execute_reply.started":"2024-06-17T08:23:50.560044Z","shell.execute_reply":"2024-06-17T08:23:50.852192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data={\n    'Age':dataset[\"Age\"],\n    'Spending_Score':dataset['Spending Score (1-100)']\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.854812Z","iopub.execute_input":"2024-06-17T08:23:50.855261Z","iopub.status.idle":"2024-06-17T08:23:50.861088Z","shell.execute_reply.started":"2024-06-17T08:23:50.855208Z","shell.execute_reply":"2024-06-17T08:23:50.859898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='Age',y='Spending_Score',data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:50.864868Z","iopub.execute_input":"2024-06-17T08:23:50.865257Z","iopub.status.idle":"2024-06-17T08:23:51.106235Z","shell.execute_reply.started":"2024-06-17T08:23:50.865226Z","shell.execute_reply":"2024-06-17T08:23:51.104913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(dataset['Age'],dataset['Spending Score (1-100)'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:51.107617Z","iopub.execute_input":"2024-06-17T08:23:51.107983Z","iopub.status.idle":"2024-06-17T08:23:51.581294Z","shell.execute_reply.started":"2024-06-17T08:23:51.107952Z","shell.execute_reply":"2024-06-17T08:23:51.579913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note**: From the above visualisation we have found that people with age between 18 to 40 has high spending score. This age group people buys more number of products than others.out which years of 18,24,32,28 buys maximum number of products.","metadata":{}},{"cell_type":"code","source":"data_1={\n    'Gender':dataset['Gender'],\n    'Spending_Score':dataset['Spending Score (1-100)']\n}\n\nplt.figure(figsize=(6,6))\nsns.boxplot(x='Gender',y='Spending_Score',data=data_1)\nplt.title(\"Spending Score Distribution by Gender\")\nplt.ylabel(\"Spending Score\")\nplt.xlabel(\"Gender\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:51.584417Z","iopub.execute_input":"2024-06-17T08:23:51.585253Z","iopub.status.idle":"2024-06-17T08:23:51.820502Z","shell.execute_reply.started":"2024-06-17T08:23:51.585217Z","shell.execute_reply":"2024-06-17T08:23:51.8194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note**:From above visualisation we had made a conclusion that female buy more products than male or female buys expensive products than male.\n* Spending score of male range between 22-70\n* Spending Score of female range between 40-75\n### FEMALES SPENDS MORE THAN MALE","metadata":{}},{"cell_type":"code","source":"data_2={\n    \"Gender\":dataset['Gender'],\n    'Annual Income':dataset['Annual Income (k$)']\n}\nplt.figure(figsize=(6,6))\nsns.boxplot(x='Gender',y='Annual Income',data=data_2)\nplt.title(\"Annual Income by Gender\")\nplt.ylabel(\"Annual Income\")\nplt.xlabel(\"Gender\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:51.82191Z","iopub.execute_input":"2024-06-17T08:23:51.822349Z","iopub.status.idle":"2024-06-17T08:23:52.060547Z","shell.execute_reply.started":"2024-06-17T08:23:51.822318Z","shell.execute_reply":"2024-06-17T08:23:52.059367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Annual Incomes of males is higher than of female.But the Spending_Score is opp of that.","metadata":{}},{"cell_type":"markdown","source":"We one outlier which has very high incomes which comes under top1% category.","metadata":{}},{"cell_type":"code","source":"dataset[dataset['Annual Income (k$)']>130]","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:52.061844Z","iopub.execute_input":"2024-06-17T08:23:52.062183Z","iopub.status.idle":"2024-06-17T08:23:52.075306Z","shell.execute_reply.started":"2024-06-17T08:23:52.062154Z","shell.execute_reply":"2024-06-17T08:23:52.074171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here it gives a clear picture of one is rich dad and one is poor dad. \n* As Rich Dad earns a lot and spend less\n* And Poor Dad earns a lot but also spends a lot.\n","metadata":{}},{"cell_type":"code","source":"# Selecting features for clusttering \nX=dataset[['Annual Income (k$)','Spending Score (1-100)']]","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:52.076552Z","iopub.execute_input":"2024-06-17T08:23:52.076876Z","iopub.status.idle":"2024-06-17T08:23:52.083196Z","shell.execute_reply.started":"2024-06-17T08:23:52.076846Z","shell.execute_reply":"2024-06-17T08:23:52.082113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:52.084553Z","iopub.execute_input":"2024-06-17T08:23:52.084902Z","iopub.status.idle":"2024-06-17T08:23:52.547545Z","shell.execute_reply.started":"2024-06-17T08:23:52.084874Z","shell.execute_reply":"2024-06-17T08:23:52.546581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elbow method to find the optimal number of clusters\nwcss=[] #within clusters sum of squares\nfor i in range(1,11):\n    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=1)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:52.548677Z","iopub.execute_input":"2024-06-17T08:23:52.549007Z","iopub.status.idle":"2024-06-17T08:23:52.777737Z","shell.execute_reply.started":"2024-06-17T08:23:52.548979Z","shell.execute_reply":"2024-06-17T08:23:52.776815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the elbow method\nplt.figure(figsize=(10,10))\nplt.plot(range(1,11),wcss,marker='o')\nplt.title(\"ELbow Method\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:52.779245Z","iopub.execute_input":"2024-06-17T08:23:52.779905Z","iopub.status.idle":"2024-06-17T08:23:53.060693Z","shell.execute_reply.started":"2024-06-17T08:23:52.779872Z","shell.execute_reply":"2024-06-17T08:23:53.059456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From above visualisation plot between number of clusters and wcss(within cluster sum of squares ","metadata":{}},{"cell_type":"markdown","source":"We have found the in elbow method where the plot shows step slope of bend in the curve will choose as the optimality clusters for the kmeans clustering algorithm.\n* optimal_clusters=5","metadata":{}},{"cell_type":"code","source":"#Applying Kmeans clustering to the dataset\nkmeans=KMeans(n_clusters=5,init='k-means++',max_iter=300,n_init=10,random_state=1)\ny_kmeans=kmeans.fit_predict(X)\n\n#Inertia (wcss)\ninertia=kmeans.inertia_\nprint(f\"Inertia:{inertia}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:53.062131Z","iopub.execute_input":"2024-06-17T08:23:53.062528Z","iopub.status.idle":"2024-06-17T08:23:53.092642Z","shell.execute_reply.started":"2024-06-17T08:23:53.06249Z","shell.execute_reply":"2024-06-17T08:23:53.090985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Magnitude of Inertia\n* A **lower inertia** values indicates that the data points are closer to thier respective cluster centroids,impying that **well defined clusters.**\n* **Higher inertia** value suggests that tha data points are more spread out **within thier clusters**.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:53.094313Z","iopub.execute_input":"2024-06-17T08:23:53.094951Z","iopub.status.idle":"2024-06-17T08:23:53.102779Z","shell.execute_reply.started":"2024-06-17T08:23:53.094913Z","shell.execute_reply":"2024-06-17T08:23:53.101542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sillhouette Scores\nsilhouette_avg=silhouette_score(X,y_kmeans)\nsilhouette_avg","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:53.104295Z","iopub.execute_input":"2024-06-17T08:23:53.10475Z","iopub.status.idle":"2024-06-17T08:23:53.122148Z","shell.execute_reply.started":"2024-06-17T08:23:53.104711Z","shell.execute_reply":"2024-06-17T08:23:53.121117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Silhoutte Score Interpretation\n* Range: The Silhoutte score ranges from -1 to 1\n     * **+1**:Indicates that the sample is far away fromm the neighbouring clusters and very close to the clusters it belongs too.This means **clusters are well separated**\n     * **0**:Indicate that the sample is on or very close to the decision boundary between two neighbouring clusters.This suggests that **clusters are not well separated and might overlap.**\n     * **-1**:Indicate that the sample might have been assigned to the **wrong cluster** as it a cluster to a neighbouring cluster than to the cluster it was assigned to.","metadata":{}},{"cell_type":"markdown","source":"## Score - 0.5539\n* A score of 0.5539 is moderately high and indicates that your clusters are reasonably well-defined.","metadata":{}},{"cell_type":"markdown","source":"### Davies Bouldin Score and calinski harabasz score\n* Davies-Bouldin Index:\n   * Interpretation:\n     1. **Lower DBI** values indicate better clustering beaause they imply that **clusters are compact** and well-separated from each other.\n     2. **Higher DBI** values suggest that the **clusters are not well separated** and may overlap significantly.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import davies_bouldin_score","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:46:48.602132Z","iopub.execute_input":"2024-06-17T08:46:48.603036Z","iopub.status.idle":"2024-06-17T08:46:48.607923Z","shell.execute_reply.started":"2024-06-17T08:46:48.602996Z","shell.execute_reply":"2024-06-17T08:46:48.606434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate Davies Bouldin Index\ndbi=davies_bouldin_score(X,y_kmeans)\nprint(f\"Davies Bouldin Score:{dbi}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:47:57.306132Z","iopub.execute_input":"2024-06-17T08:47:57.306884Z","iopub.status.idle":"2024-06-17T08:47:57.31663Z","shell.execute_reply.started":"2024-06-17T08:47:57.306847Z","shell.execute_reply":"2024-06-17T08:47:57.315534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Specific Score of 0.5726\n* Good Clustering: A score of 0.5726 is **relatively low** ,which means your clusters are **fairly compact and well-separated from one another**.This indicates **good clusterring performance**.","metadata":{}},{"cell_type":"markdown","source":"### Calinski-Harabasz Index\nCalinski-Harabsz index also known as variance ratio criterion is another metric used to evaluate the quality of clustering algorithm.\n* **Higher CHI values** indicate better clustering because they imply that the clusters are well separated and compact.\n* **Lower CH index** values suggest poorer clustering with more overlap and less distinct clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import calinski_harabasz_score","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:55:28.063217Z","iopub.execute_input":"2024-06-17T08:55:28.06402Z","iopub.status.idle":"2024-06-17T08:55:28.068758Z","shell.execute_reply.started":"2024-06-17T08:55:28.063983Z","shell.execute_reply":"2024-06-17T08:55:28.067545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate Calinski_harabasz score\nch_index=calinski_harabasz_score(X,y_kmeans)\nprint(f'Calinski Harabasz Index:{ch_index}')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:56:44.926263Z","iopub.execute_input":"2024-06-17T08:56:44.92723Z","iopub.status.idle":"2024-06-17T08:56:44.934676Z","shell.execute_reply.started":"2024-06-17T08:56:44.927192Z","shell.execute_reply":"2024-06-17T08:56:44.933596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Specific Score of 247.3589\n* Good Clustering: A score of 247.3589 is relatively high and suggests that the clusters are dense (compact) and well-separated from each other.This indicates good clustering algorithm.","metadata":{}},{"cell_type":"markdown","source":"# Visualize the output","metadata":{}},{"cell_type":"code","source":"dataset['Cluster']=y_kmeans","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:53.123325Z","iopub.execute_input":"2024-06-17T08:23:53.123674Z","iopub.status.idle":"2024-06-17T08:23:53.129455Z","shell.execute_reply.started":"2024-06-17T08:23:53.123646Z","shell.execute_reply":"2024-06-17T08:23:53.128454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the clusters\nplt.figure(figsize=(10,10))\nsns.scatterplot(data=dataset,x='Annual Income (k$)',y='Spending Score (1-100)',hue='Cluster',palette='viridis',s=100,alpha=0.7)\n\n# Plotting clusters centers\ncenter=kmeans.cluster_centers_\nplt.scatter(center[:,0],center[:,1],c='red',s=200,marker='X',label='Centroids')\n\nplt.title(\"Clusters of Customers\")\nplt.xlabel(\"Annual Income (k$)\")\nplt.ylabel(\"Spending Score (1-100)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T08:23:53.130729Z","iopub.execute_input":"2024-06-17T08:23:53.131061Z","iopub.status.idle":"2024-06-17T08:23:53.617472Z","shell.execute_reply.started":"2024-06-17T08:23:53.131017Z","shell.execute_reply":"2024-06-17T08:23:53.616329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_1=dataset.drop(['Gender'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:24:53.148606Z","iopub.execute_input":"2024-06-17T09:24:53.149534Z","iopub.status.idle":"2024-06-17T09:24:53.157053Z","shell.execute_reply.started":"2024-06-17T09:24:53.149496Z","shell.execute_reply":"2024-06-17T09:24:53.155663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering,DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\n#Normalise the data\nscaler=StandardScaler()\nX_scaled=scaler.fit_transform(dataset_1)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:26:25.924903Z","iopub.execute_input":"2024-06-17T09:26:25.925329Z","iopub.status.idle":"2024-06-17T09:26:25.937564Z","shell.execute_reply.started":"2024-06-17T09:26:25.925296Z","shell.execute_reply":"2024-06-17T09:26:25.935882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Other Clustering Methods\n## 1.Agglomerative Hierarchical clusterring \nAgglomerative Hierarchical clustering is a bottom up apporach where each obersvation starts in its own cluster and pairs of clusters are merged as one moves up the hierarchy. ","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering \n\n#Perform Agglomerative Hierarchical Clustering\nagg_cluster=AgglomerativeClustering(n_clusters=5)\nclusters_agg=agg_cluster.fit_predict(X_scaled)\n\n#Add the cluster labels to the original DataFrame\ndataset_1['cluster_agg']=clusters_agg","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:39:48.956834Z","iopub.execute_input":"2024-06-17T09:39:48.957902Z","iopub.status.idle":"2024-06-17T09:39:48.965987Z","shell.execute_reply.started":"2024-06-17T09:39:48.957857Z","shell.execute_reply":"2024-06-17T09:39:48.964787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise the clusters\nplt.figure(figsize=(10,6))\nsns.scatterplot(data=dataset_1,x='Annual Income (k$)',y='Spending Score (1-100)',hue='cluster_agg',palette='viridis',s=100,alpha=0.7)\nplt.title(\"Customer Segmentation - Agglomerative Clustering\")\nplt.xlabel(\"Annual Income (k$)\")\nplt.ylabel(\"Spending Score (1-100)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:40:12.269807Z","iopub.execute_input":"2024-06-17T09:40:12.270204Z","iopub.status.idle":"2024-06-17T09:40:12.695622Z","shell.execute_reply.started":"2024-06-17T09:40:12.270171Z","shell.execute_reply":"2024-06-17T09:40:12.694249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the clustering performance\nsilhouette_avg_agg=silhouette_score(X_scaled,clusters_agg)\ndavies_bouldin_avg_agg=davies_bouldin_score(X_scaled,clusters_agg)\ncalinski_harabasz_avg_agg=calinski_harabasz_score(X_scaled,clusters_agg)\n\nprint(f'Agglomerative Clustering - Silhouette Score:{silhouette_avg_agg}')\nprint(f\"Agglomerative Clustering - Devies Bouldin Score:{davies_bouldin_avg_agg}\")\nprint(f\"Agglomerative Clustering - Calinskti Harabasz Score:{calinski_harabasz_avg_agg}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:45:28.851641Z","iopub.execute_input":"2024-06-17T09:45:28.85203Z","iopub.status.idle":"2024-06-17T09:45:28.865263Z","shell.execute_reply.started":"2024-06-17T09:45:28.852001Z","shell.execute_reply":"2024-06-17T09:45:28.863908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insight \nFrom above evaluation we only conclude that clusters has moderately separation between the clusters but not as distinct as ideally desired.","metadata":{}},{"cell_type":"markdown","source":"### Dendogram of Aggolmerative Clustering ","metadata":{}},{"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram,linkage\n\n#Select the relvant columns for clustering \nX=dataset[['Age','Annual Income (k$)','Spending Score (1-100)']]\n\n#Normalise the data\nscaler=StandardScaler()\nX_scaled=scaler.fit_transform(X)\n\n# Perform Agglomerative Clustering to obtain the linkage matrix\nZ=linkage(X_scaled,method='ward',metric='euclidean')\n\n#Plot the dendogram\nplt.figure(figsize=(10,6))\ndendrogram(Z,labels=dataset.index.values,orientation='top')\nplt.title(\"Dendogram for Agglomerative Clustering\")\nplt.xlabel(\"Customer Index\")\nplt.ylabel(\"Distance\")\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:10:49.825935Z","iopub.execute_input":"2024-06-17T10:10:49.826359Z","iopub.status.idle":"2024-06-17T10:10:52.513027Z","shell.execute_reply.started":"2024-06-17T10:10:49.826325Z","shell.execute_reply":"2024-06-17T10:10:52.511785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.DBSCAN (DENSITY BASED SPATIAL CLUSTERING APPICATION AND NOISE)\nDBSCAN is a density based clustering method that groups together points that are close to each other based on a distance measurement (epsilon) and a minimum number of points(min_samples) ","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\n#Perform DBSCAN Clustering\ndbscan=DBSCAN(eps=0.5,min_samples=5)\nclusters_dbscan=dbscan.fit_predict(X_scaled)\n\n#Add the cluster labels to the original dataframe\ndataset_1['Cluster_DBSCAN']=clusters_dbscan","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:16:51.734726Z","iopub.execute_input":"2024-06-17T10:16:51.735139Z","iopub.status.idle":"2024-06-17T10:16:51.744668Z","shell.execute_reply.started":"2024-06-17T10:16:51.735103Z","shell.execute_reply":"2024-06-17T10:16:51.743444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise the clusters \nplt.figure(figsize=(10,10))\nsns.scatterplot(data=dataset_1,x='Annual Income (k$)',y='Spending Score (1-100)',hue='Cluster_DBSCAN',palette='viridis',s=100,alpha=0.7)\nplt.title(\"Customer Segmentation - DBSCAN Clustering\")\nplt.xlabel(\"Annual Income (k$)\")\nplt.ylabel(\"Spending Score (1-100)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:21:06.724146Z","iopub.execute_input":"2024-06-17T10:21:06.724569Z","iopub.status.idle":"2024-06-17T10:21:07.229094Z","shell.execute_reply.started":"2024-06-17T10:21:06.724533Z","shell.execute_reply":"2024-06-17T10:21:07.227911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It does not properly distinct clusters because a methods it make custers on basis of sample points nearer to each other.\n* Those datapoints are closer to another comes under same cluster but here see overlapping of points of different clusters.","metadata":{}},{"cell_type":"code","source":"#Evaluate Clustering performance \ndavies_bouldin_avg_dbscan=davies_bouldin_score(X_scaled,clusters_dbscan)\ncalinski_harabasz_avg_dbscan=calinski_harabasz_score(X_scaled,clusters_dbscan)\n\nprint(f\"Davies Bouldin Score:{davies_bouldin_avg_dbscan}\")\nprint(f\"Calinski Harabasz Score:{calinski_harabasz_avg_dbscan}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:27:24.921487Z","iopub.execute_input":"2024-06-17T10:27:24.921932Z","iopub.status.idle":"2024-06-17T10:27:24.93431Z","shell.execute_reply.started":"2024-06-17T10:27:24.921896Z","shell.execute_reply":"2024-06-17T10:27:24.93302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Davies Bouldin Score measures the avg similarity between each cluster and its most similar cluster.\n* A score of 1.756 suggests that there **moderate overlap** or similarity between clusters indicating that the **clusters might not be well-separated** as desired.","metadata":{}},{"cell_type":"markdown","source":"### Check if the dataset given forms a gaussian distribution","metadata":{}},{"cell_type":"code","source":"#Visual Inspection using Histogram and Density plots\nplt.figure(figsize=(500,6))\nfor i,col in enumerate(dataset_1.columns):\n    plt.subplot(1,len(dataset_1),i+1)\n    sns.histplot(dataset_1[col],kde=True)\n    plt.title(col)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:50:27.345227Z","iopub.execute_input":"2024-06-17T10:50:27.34572Z","iopub.status.idle":"2024-06-17T10:50:29.096569Z","shell.execute_reply.started":"2024-06-17T10:50:27.345684Z","shell.execute_reply":"2024-06-17T10:50:29.095323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2.Quantile-Quantile plot\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n#Example Q-Q plot for one column \nstats.probplot(dataset_1['Annual Income (k$)'],dist='norm',plot=plt)\nplt.title(\"Q-Q plot for annual Income\")\nplt.xlabel(\"Theoritical Quantile\")\nplt.ylabel(\"Sample Quantiles\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:53:12.042467Z","iopub.execute_input":"2024-06-17T10:53:12.043492Z","iopub.status.idle":"2024-06-17T10:53:12.319563Z","shell.execute_reply.started":"2024-06-17T10:53:12.043453Z","shell.execute_reply":"2024-06-17T10:53:12.318424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Points lying close to the diagonal line indicate that the data follows a normal distribution","metadata":{}},{"cell_type":"code","source":"# 3.Statistical test\nfrom scipy.stats import shapiro,normaltest\n\n#Example of shapiro-wilk test for normally \nstats,p=shapiro(dataset_1['Annual Income (k$)'])\nprint(f\"Shapiro-Wilk Test - p-values:{p}\")\n\n#Example of kolmogorov-smirnov test for normality\nstats,p=normaltest(dataset_1['Annual Income (k$)'])\nprint(f\"Kolmogorov-smirnov Test - p-values:{p}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:58:23.037002Z","iopub.execute_input":"2024-06-17T10:58:23.037533Z","iopub.status.idle":"2024-06-17T10:58:23.051995Z","shell.execute_reply.started":"2024-06-17T10:58:23.03749Z","shell.execute_reply":"2024-06-17T10:58:23.050475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Shapiro-Wilk test is generally more sensitive to deviations from normality compared to the kolmogorov-Smirnov test,especially with small sample sizes.\n* It assesses how tell the data follows a normal distribution shape-wise","metadata":{}},{"cell_type":"markdown","source":"## 3.Spectral Clustering \nSpectral clustering uses the eigenvalues of similarity matrix to reduce the dimensionality of the data before in fewer dimensions.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\n\n#Perform Spectrl clustering\nspectral_cluster=SpectralClustering(n_clusters=5,affinity='nearest_neighbors',random_state=1)\nclusters_spectral=spectral_cluster.fit_predict(X_scaled)\n\n#Add the clusters labels to the original dataframe\ndataset_1['Cluster_spectral']=clusters_spectral\n\n#Visualise the clusters\nplt.figure(figsize=(10,10))\nsns.scatterplot(data=dataset_1,x='Annual Income (k$)',y='Spending Score (1-100)',hue='Cluster_spectral',palette='viridis',s=100,alpha=0.7)\nplt.title(\"Customer Segmentaion - Spectral Clustering \")\nplt.xlabel(\"Annual Income (k$)\")\nplt.ylabel(\"Spending Score (1-100)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:21:46.586767Z","iopub.execute_input":"2024-06-17T11:21:46.587269Z","iopub.status.idle":"2024-06-17T11:21:47.107412Z","shell.execute_reply.started":"2024-06-17T11:21:46.587228Z","shell.execute_reply":"2024-06-17T11:21:47.106239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion:\n* Now concluding that we have applied many unsupervised learning algorithms like K-means clustering,Agglomerative and Spectral Clustering.\n* Evaluate models performance various metrics like silhouette Score, Davies Bouldin Score and calinski Harabasz Score and etc.\n* Visualise each every aspects and features of dataset using plots like scatterplot,histplot,and etc.\n* Analysis 0f different features and how they are dependent of each other and which are most important ones.\n* Distinct clusters are formed using k-means clustring alogritm. We have found that optimal clusters for the dataset must be 5 using elbow method and also visualise it.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}